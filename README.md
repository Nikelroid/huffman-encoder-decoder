# Huffman Text Compression (Lossless & Lossy)

![Java](https://img.shields.io/badge/Java-18%2B-ED8B00?logo=openjdk&logoColor=white)
![IntelliJ IDEA](https://img.shields.io/badge/IntelliJ_IDEA-Project-000000?logo=intellij-idea&logoColor=white)
![Algorithm](https://img.shields.io/badge/Algorithm-Huffman-blue)
![License](https://img.shields.io/badge/License-MIT-green)


## üìù Project Description
Compression is a fundamental concept in information theory, crucial for reducing storage space and optimizing bandwidth usage. This project provides a complete **Huffman Encoder-Decoder** system for text files, implemented in Java.

What makes this project unique is its dual-mode capability:
1.  **Lossless Compression:** Compresses text without losing any data, allowing for a perfect reconstruction of the original file.
2.  **Lossy Compression:** Introduces a "Lossy Factor" that selectively removes low-frequency characters from the Huffman tree, replacing them with a placeholder (e.g., `~`). This significantly reduces the dictionary size and the overall file size while maintaining the general structure and readability of the text.

This project was developed as part of the Data Structures course at **Sharif University of Technology**.

## ‚ú® Key Features
* **Customizable Loss Factor:** Users can define a loss percentage (0-100). A factor of `0` runs standard lossless compression. Higher factors aggressively prune the Huffman tree, trading minor data loss for higher compression ratios.
* **7-Bit Block Optimization:** Instead of standard 8-bit bytes, this implementation maps binary streams into 7-bit blocks converted to ASCII characters. This avoids the overhead of Java reserving the 8th bit, ensuring tighter compression.
* **Self-Contained Archives:** The Huffman dictionary (metadata) is serialized and stored in the header of the compressed file. This allows the decoder to reconstruct the original text using *only* the compressed file, without needing a separate key file.
* **Special Character Handling:** To prevent file reading errors during decoding, standard Line Feed (LF) and Carriage Return (CR) characters (ASCII 10 and 13) are encoded as extended ASCII values (129 and 128), ensuring data integrity.
* **Statistical Analysis:** The repository includes data on how different text types (homogeneous, heterogeneous, normal) respond to various compression levels.

## üìÇ Project Structure & File Descriptions

### Source Code (`src/`)
* **`Encoding.java`**: The core logic for compression.
    * **Histogram Generation:** Counts character frequencies.
    * **Tree Construction:** Builds the Huffman tree using a `PriorityQueue`.
    * **Lossy Logic:** If a loss factor is set, it filters out valid characters based on frequency before building the tree.
    * **Serializer:** Converts the dictionary and the compressed bitstream into a single string of 7-bit ASCII characters.
* **`Decoding.java`**: The core logic for decompression.
    * **Header Parsing:** Reads the metadata from the start of the file to reconstruct the Huffman dictionary.
    * **Bitstream Traversal:** Iterates through the encoded body, traversing the reconstructed Huffman tree to find leaf nodes and output characters.
* **`Main.java`**: The entry point of the application. It handles file I/O, accepts user input for the loss factor, and orchestrates the encoding/decoding flow.

### Resources (`resource/`)
* **`Input[X].txt`**: A variety of raw text files used for testing, ranging from short sentences to long paragraphs and repetitive text patterns.
* **`encoded[X].txt`**: The output files generated by the encoder.
* **`decoded[X].txt`**: The output files generated by the decoder (used to verify accuracy against the input).
* **`Project-charts.xlsx...`**: CSV files containing experimental results, comparing compression ratios across different file sizes and loss factors.

## üöÄ Installation and Usage

### Prerequisites
* Java Development Kit (JDK) 18 or higher.
* An IDE like IntelliJ IDEA is recommended but not required.

### Step-by-Step Guide
1.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/YOUR_USERNAME/Huffman-Encoder-Decoder.git](https://github.com/YOUR_USERNAME/Huffman-Encoder-Decoder.git)
    cd Huffman-Encoder-Decoder
    ```

2.  **Configure Input File:**
    Open `src/Main.java` and modify the `INPUT` variable to point to the text file you wish to compress (e.g., `input12.txt` inside the `resource` folder).

3.  **Compile and Run:**
    Compile the Java files and run the `Main` class.
    ```bash
    javac src/*.java
    java src.Main
    ```

4.  **Select Compression Mode:**
    The program will prompt you to enter a **Lossy Factor**:
    * Enter **`0`** for **Lossless Compression** (perfect reconstruction).
    * Enter a number between **`1` and `100`** for **Lossy Compression**. Higher numbers result in smaller files but more "invalid" characters (replaced by `~`).

5.  **Check Output:**
    * The compressed file will be saved as `encoded[X].txt`.
    * The decompressed text will be saved as `decoded[X].txt`.

## üë• Authors
* **Nima Kelidari**: Lead Developer & Implementation.
* **Masoud Alipour**: Project Conceptualization & Analysis.
* **Mohammadreza Pournader**: Project Conceptualization & Analysis.

## ü§ù Contributing
Contributions are welcome! If you have ideas for optimizing the tree structure or improving the 7-bit serialization logic:
1.  Fork the project.
2.  Create your feature branch (`git checkout -b feature/NewFeature`).
3.  Commit your changes (`git commit -m 'Add some NewFeature'`).
4.  Push to the branch (`git push origin feature/NewFeature`).
5.  Open a Pull Request.

## üìÑ License
This project is open-source and available under the [MIT License](LICENSE).
